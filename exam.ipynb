{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "exam.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOv+Y9Cuj0kRsQ85N6UGIYw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sourabhsdx/waylyf/blob/master/exam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT-GLJK6FFvy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "04eb516b-fb46-4976-feec-bc0ff22bef78"
      },
      "source": [
        "import math \n",
        "from nltk.corpus import wordnet\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "def read_data(variable):\n",
        "      variable = variable\n",
        "      #print(type(variable))\n",
        "      #tokenized_sent = word_tokenize(variable)\n",
        "      tokenizer = RegexpTokenizer(r'\\w+')\n",
        "      tokenized_sent = tokenizer.tokenize(variable)\n",
        "      stop_words=set(stopwords.words(\"english\"))\n",
        "      filtered_stem_sent=[]\n",
        "      porter = PorterStemmer()\n",
        "      lemmatizer = WordNetLemmatizer()\n",
        "      for w in tokenized_sent:\n",
        "        w = w.lower()\n",
        "        if w not in stop_words:\n",
        "          #new_w = porter.stem(w)\n",
        "          new_w = lemmatizer.lemmatize(w,pos =\"a\")\n",
        "          if new_w == w:\n",
        "            new_w = lemmatizer.lemmatize(w,pos =\"n\")\n",
        "            if new_w == w:\n",
        "              new_w = lemmatizer.lemmatize(w,pos =\"v\")\n",
        "              if new_w == w:\n",
        "                new_w = lemmatizer.lemmatize(w,pos =\"r\")\n",
        "          filtered_stem_sent.append(new_w)\n",
        "      return (filtered_stem_sent)\n",
        "\n",
        "\n",
        "def _create_frequency_matrix(sentences, common_ans):\n",
        "    frequency_matrix = {}\n",
        "    freq_table = []\n",
        "    result_list = list(set(common_ans) - set(sentences))\n",
        "    for word in sentences:\n",
        "      if word in freq_table:\n",
        "        frequency_matrix[word] = frequency_matrix[word]+1\n",
        "      else:\n",
        "        frequency_matrix[word] = 1\n",
        "        freq_table.append(word)\n",
        "      for w in result_list:\n",
        "        synonyms = []\n",
        "        for syn in wordnet.synsets(w):\n",
        "          for l in syn.lemmas():\n",
        "            synonyms.append(l.name())\n",
        "        set_syn = set(synonyms)\n",
        "        frequency_matrix[w] = 0\n",
        "        for w1 in set_syn:\n",
        "          if w1 in freq_table:\n",
        "            frequency_matrix[w] = 1\n",
        "            freq_table.append(word)\n",
        "            break\n",
        "\n",
        "    return(frequency_matrix)\n",
        "\n",
        "def _create_tf_matrix(frequency_matrix, T_length):\n",
        "  TF = {}\n",
        "  for words in frequency_matrix:\n",
        "    TF[words] = frequency_matrix[words]/T_length\n",
        "  return (TF)\n",
        "\n",
        "def _create_frequency_matrix_for_idf(common_ans, aut_ans, stu_ans):\n",
        "    idf_matrix = {}\n",
        "    for words in common_ans:\n",
        "      if words in aut_ans:\n",
        "        if words in stu_ans:\n",
        "          idf_matrix[words] = 2\n",
        "        else:\n",
        "          idf_matrix[words] = 1\n",
        "      else:\n",
        "        idf_matrix[words] = 1\n",
        "    return idf_matrix\n",
        "\n",
        "def _create_idf(idf_tb):\n",
        "  idf = {}\n",
        "  for words in idf_tb:\n",
        "    idf[words] = math.log(1+(2/(idf_tb[words])), 10) \n",
        "  return idf\n",
        "def _create_weight(tf,idf):\n",
        "  weight = {}\n",
        "  for i in tf:\n",
        "    weight[i] = tf[i]*idf[i]\n",
        "  return weight\n",
        "\n",
        "author_ans = \"I am a good boy \"\n",
        "student_ans = \"i am a soun boy\"\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "def main(author_ans,student_ans):\n",
        "  aut_ans = read_data(author_ans)\n",
        "  stu_ans = read_data(student_ans)\n",
        "  common_ans = []\n",
        "  for i in aut_ans:\n",
        "    if i not in common_ans:\n",
        "      common_ans.append(i)\n",
        "  for i in stu_ans:\n",
        "    if i not in common_ans:\n",
        "      common_ans.append(i)\n",
        "  aut_frq_tb = _create_frequency_matrix(aut_ans, common_ans)\n",
        "  stu_frq_tb = _create_frequency_matrix(stu_ans, common_ans)\n",
        "  #length = len(aut_ans)\n",
        "  # print(\"frq table\", aut_frq_tb)\n",
        "  # print(\"frq table\",stu_frq_tb)\n",
        "  aut_tf = _create_tf_matrix(aut_frq_tb,len(aut_ans))\n",
        "  stu_tf = _create_tf_matrix(stu_frq_tb,len(stu_ans))\n",
        "  # print(aut_ans)\n",
        "  # print(stu_ans)\n",
        "  # print(common_ans)\n",
        "  idf_tb = _create_frequency_matrix_for_idf(common_ans, aut_ans, stu_ans)\n",
        "  #print(idf_tb)\n",
        "  idf = _create_idf(idf_tb)\n",
        "  aut_weight = _create_weight(aut_tf, idf)\n",
        "  stu_weight = _create_weight(stu_tf, idf)\n",
        "  # print(\"tf of author : \",aut_tf)\n",
        "  # print(\"tf of student : \",stu_tf)\n",
        "  # print(\"idf : \",idf)\n",
        "  # print(aut_weight)\n",
        "  # print(stu_weight)\n",
        "  x = []\n",
        "  y = []\n",
        "  for words in common_ans:\n",
        "    #print(words,aut_weight[words],words,stu_weight[words])\n",
        "    x.append(aut_weight[words])\n",
        "    y.append(stu_weight[words])\n",
        "  x1 = np.array([x])\n",
        "  y1 = np.array([y])\n",
        "  print(cosine_similarity(x1,y1))\n",
        "main(author_ans,student_ans)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[[0.28472944]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}